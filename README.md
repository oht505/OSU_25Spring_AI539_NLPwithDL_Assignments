
---

## HW1 â€“ Word Vectors: Distributed Representations of Words

- **Goal**: Explore tokenization and distributional representations of words (word2vec)
- **Tools**: Python
- **Highlights**:
  - Frequency plots for top tokens
  - Distribution analysis

<img width="1400" height="500" alt="Frequency Distribution and Cumulative Fraction Covered" src="https://github.com/user-attachments/assets/d20c8acb-db49-492f-9ee9-79969ead2833" />

<img width="640" height="480" alt="glove_marked" src="https://github.com/user-attachments/assets/1d039a69-bc79-4d80-9ff0-685b0853829f" />

See: [`HW1/`](./HW1/)

---

## HW2 â€“ Processing Sequences with Recurrent Neural Networks 

- **Goal**: Train recurrent neural networks for sequence classification tasks - a simple parity problem and a part-of-speech tagging application.
- **Tools**: NumPy, matplotlib
- **Highlights**:
  - Parity Recurrent Neural Network
  - Bidirectional GRUs

![HW2 Output](./images/HW2_perplexity_plot.png)

See: [`HW2/`](./HW2/)

---

## HW3 â€“ Sequence-to-Sequence Models with Attention

- **Goal**: Train recurrent neural networks for a conditional sequence generation task - translating German to English
- **Tools**: PyTorch
- **Highlights**:
  - Attention model (Encoder, Decoder)
  - Comparison attention model with none and mean models

![HW3 Output](./images/HW3_finetune_sample.png)

See: [`HW3/`](./HW3/)

---

## HW4 â€“ Transformer-based Language Models

- **Goal**: Implement transformer-based language models
- **Tools**: Pytorch
- **Highlights**:
  - Transformer-based Autoregressive Language Models
  - Decoding Strategies for Autoregressive Language Models

![HW4 Output](./images/HW4_privacy_results.png)

 See: [`HW4/`](./HW4/)

---

## Author

**Hyuntaek Oh**  
M.S. in Artificial Intelligence â€“ Oregon State University  

---

## ðŸ“Œ Notes

- All assignments were completed individually as part of AI539 (Spring 2025).
- For questions, feel free to contact me via GitHub or email.
