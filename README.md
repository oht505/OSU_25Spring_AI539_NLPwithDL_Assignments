
---

## HW1 â€“ Word Vectors: Distributed Representations of Words

- **Goal**: Explore tokenization and distributional representations of words (word2vec)
- **Tools**: Python
- **Highlights**:
  - Frequency plots for top tokens
  - Distribution analysis

![HW1 Output](./HW1/Frequency%20Distribution%20and%20Cumulative%20Fraction%20Covered.png_word_freq_plot.png)

See: [`HW1/`](./HW1/)

---

## HW2 â€“ Processing Sequences with Recurrent Neural Networks 

- **Goal**: Train recurrent neural networks for sequence classification tasks - a simple parity problem and a part-of-speech tagging application.
- **Tools**: NumPy, matplotlib
- **Highlights**:
  - Parity Recurrent Neural Network
  - Bidirectional GRUs

![HW2 Output](./images/HW2_perplexity_plot.png)

See: [`HW2/`](./HW2/)

---

## HW3 â€“ Sequence-to-Sequence Models with Attention

- **Goal**: Train recurrent neural networks for a conditional sequence generation task - translating German to English
- **Tools**: PyTorch
- **Highlights**:
  - Attention model (Encoder, Decoder)
  - Comparison attention model with none and mean models

![HW3 Output](./images/HW3_finetune_sample.png)

See: [`HW3/`](./HW3/)

---

## HW4 â€“ Transformer-based Language Models

- **Goal**: Implement transformer-based language models
- **Tools**: Pytorch
- **Highlights**:
  - Transformer-based Autoregressive Language Models
  - Decoding Strategies for Autoregressive Language Models

![HW4 Output](./images/HW4_privacy_results.png)

 See: [`HW4/`](./HW4/)

---

## Author

**Hyuntaek Oh**  
M.S. in Artificial Intelligence â€“ Oregon State University  

---

## ðŸ“Œ Notes

- All assignments were completed individually as part of AI539 (Spring 2025).
- For questions, feel free to contact me via GitHub or email.
