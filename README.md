
---

## HW1 â€“ Word Vectors: Distributed Representations of Words

- **Goal**: Explore tokenization and distributional representations of words (word2vec)
- **Tools**: Python
- **Highlights**:
  - Frequency plots for top tokens
  - Distribution analysis

<img width="1400" height="500" alt="Frequency Distribution and Cumulative Fraction Covered" src="https://github.com/user-attachments/assets/d20c8acb-db49-492f-9ee9-79969ead2833" />

<img width="640" height="480" alt="glove_marked" src="https://github.com/user-attachments/assets/1d039a69-bc79-4d80-9ff0-685b0853829f" />

See: [`HW1/`](./HW1/)

---

## HW2 â€“ Processing Sequences with Recurrent Neural Networks 

- **Goal**: Train recurrent neural networks for sequence classification tasks - a simple parity problem and a part-of-speech tagging application.
- **Tools**: NumPy, matplotlib
- **Highlights**:
  - Parity Recurrent Neural Network
  - Bidirectional GRUs

<img width="3792" height="1992" alt="hiddenDimension" src="https://github.com/user-attachments/assets/3d5c69a5-440d-42b7-b317-a1b6dda65f2b" />
<img width="293" height="188" alt="q12_1" src="https://github.com/user-attachments/assets/cc35ff93-ac94-44a8-807a-68ca9fb6106b" />
<img width="215" height="223" alt="q12_2" src="https://github.com/user-attachments/assets/d967033d-1fde-4fb9-acc4-761f16edb369" />
<img width="241" height="185" alt="q12_3" src="https://github.com/user-attachments/assets/f59b76f5-0efe-4000-a0c9-eccaf782983d" />

See: [`HW2/`](./HW2/)

---

## HW3 â€“ Sequence-to-Sequence Models with Attention

- **Goal**: Train recurrent neural networks for a conditional sequence generation task - translating German to English
- **Tools**: PyTorch
- **Highlights**:
  - Attention model (Encoder, Decoder)
  - Comparison attention model with none and mean models

<img width="714" height="254" alt="dog" src="https://github.com/user-attachments/assets/9d8a549d-a793-4fd7-8ecb-e6f26560d4b1" />
<img width="740" height="285" alt="dog_mean_visual" src="https://github.com/user-attachments/assets/cec597fc-2019-45ff-bc82-123598f2724c" />
<img width="734" height="265" alt="dog_none_visual" src="https://github.com/user-attachments/assets/e574e5d9-efd4-46bc-809b-bd0c58f1b7db" />
<img width="749" height="305" alt="dog_zeroDropOut" src="https://github.com/user-attachments/assets/6791d704-c9cb-4278-aa50-3d14d48de730" />

See: [`HW3/`](./HW3/)

---

## HW4 â€“ Transformer-based Language Models

- **Goal**: Implement transformer-based language models
- **Tools**: Pytorch
- **Highlights**:
  - Transformer-based Autoregressive Language Models
  - Decoding Strategies for Autoregressive Language Models

![2_argmax](https://github.com/user-attachments/assets/ba12681c-63db-40df-8bf2-131ce7ffaa55)
![2_nucleus](https://github.com/user-attachments/assets/8e009622-6178-4ef5-b4f2-fb6030d060fd)
![2_sample](https://github.com/user-attachments/assets/b94f986b-7382-4aeb-888b-e9424cc5d66a)

 See: [`HW4/`](./HW4/)

---

## Author

**Hyuntaek Oh**  
M.S. in Artificial Intelligence â€“ Oregon State University  

---

## ðŸ“Œ Notes

- All assignments were completed individually as part of AI539 (Spring 2025).
- For questions, feel free to contact me via GitHub or email.
